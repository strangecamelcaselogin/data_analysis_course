\documentclass[a4paper,12pt]{article}

\usepackage[utf8x]{inputenc}
\usepackage[english, russian]{babel}

\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{misccorr}
\usepackage{indentfirst}
\usepackage{amsmath}
%\usepackage{fancynum}


\usepackage{listings}
\usepackage{xcolor}

\usepackage{fullpage}

\usepackage[labelsep=endash,
		    margin=10pt, 
		    justification = centerlast, 
		    format = hang,
		    singlelinecheck=false
		    ]{caption}

\exhyphenpenalty=10000
\doublehyphendemerits=10000
\finalhyphendemerits=5000

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\newcommand{\tracking}[2]{#2}
\input{letterspacing.tex}\renewcommand{\tracking}[2]{\mbox{\letterspace to #1\naturalwidth{#2}}}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=t,
    keepspaces=true,
    numbers=left,
    numbersep=10pt,
    showspaces=false,
    showstringspaces=false
    showtabs=false,
    tabsize=4,
    frame=tb
}
 
\lstset{style=mystyle}

\usepackage{color}
\usepackage{xcolor}
\usepackage{listings}
 
% Цвета для кода
 
\definecolor{string}{HTML}{B40000} % цвет строк в коде
\definecolor{comment}{HTML}{008000} % цвет комментариев в коде
\definecolor{keyword}{HTML}{1A00FF} % цвет ключевых слов в коде
\definecolor{morecomment}{HTML}{8000FF} % цвет include и других элементов в коде
\definecolor{сaptiontext}{HTML}{FFFFFF} % цвет текста заголовка в коде
\definecolor{сaptionbk}{HTML}{999999} % цвет фона заголовка в коде
\definecolor{bk}{HTML}{FFFFFF} % цвет фона в коде
\definecolor{frame}{HTML}{999999} % цвет рамки в коде
\definecolor{brackets}{HTML}{B40000} % цвет скобок в коде
 

%%% Отображение кода %%%
 
% Настройки отображения кода
 
\lstset{
	%morekeywords={*,...}, % если хотите добавить ключевые слова, то добавляйте	 
	% Настройки отображения     
	breaklines=false, % Перенос длинных строк
	% Для отображения русского языка
	extendedchars=true,
	literate={Ö}{{\"O}}1
	{Ä}{{\"A}}1
	{Ü}{{\"U}}1
	{ß}{{\ss}}1
	{ü}{{\"u}}1
	{ä}{{\"a}}1
	{ö}{{\"o}}1
	{~}{{\textasciitilde}}1
	{а}{{\selectfont\char224}}1
	{б}{{\selectfont\char225}}1
	{в}{{\selectfont\char226}}1
	{г}{{\selectfont\char227}}1
	{д}{{\selectfont\char228}}1
	{е}{{\selectfont\char229}}1
	{ё}{{\"e}}1
	{ж}{{\selectfont\char230}}1
	{з}{{\selectfont\char231}}1
	{и}{{\selectfont\char232}}1
	{й}{{\selectfont\char233}}1
	{к}{{\selectfont\char234}}1
	{л}{{\selectfont\char235}}1
	{м}{{\selectfont\char236}}1
	{н}{{\selectfont\char237}}1
	{о}{{\selectfont\char238}}1
	{п}{{\selectfont\char239}}1
	{р}{{\selectfont\char240}}1
	{с}{{\selectfont\char241}}1
	{т}{{\selectfont\char242}}1
	{у}{{\selectfont\char243}}1
	{ф}{{\selectfont\char244}}1
	{х}{{\selectfont\char245}}1
	{ц}{{\selectfont\char246}}1
	{ч}{{\selectfont\char247}}1
	{ш}{{\selectfont\char248}}1
	{щ}{{\selectfont\char249}}1
	{ъ}{{\selectfont\char250}}1
	{ы}{{\selectfont\char251}}1
	{ь}{{\selectfont\char252}}1
	{э}{{\selectfont\char253}}1
	{ю}{{\selectfont\char254}}1
	{я}{{\selectfont\char255}}1
	{А}{{\selectfont\char192}}1
	{Б}{{\selectfont\char193}}1
	{В}{{\selectfont\char194}}1
	{Г}{{\selectfont\char195}}1
	{Д}{{\selectfont\char196}}1
	{Е}{{\selectfont\char197}}1
	{Ё}{{\"E}}1
	{Ж}{{\selectfont\char198}}1
	{З}{{\selectfont\char199}}1
	{И}{{\selectfont\char200}}1
	{Й}{{\selectfont\char201}}1
	{К}{{\selectfont\char202}}1
	{Л}{{\selectfont\char203}}1
	{М}{{\selectfont\char204}}1
	{Н}{{\selectfont\char205}}1
	{О}{{\selectfont\char206}}1
	{П}{{\selectfont\char207}}1
	{Р}{{\selectfont\char208}}1
	{С}{{\selectfont\char209}}1
	{Т}{{\selectfont\char210}}1
	{У}{{\selectfont\char211}}1
	{Ф}{{\selectfont\char212}}1
	{Х}{{\selectfont\char213}}1
	{Ц}{{\selectfont\char214}}1
	{Ч}{{\selectfont\char215}}1
	{Ш}{{\selectfont\char216}}1
	{Щ}{{\selectfont\char217}}1
	{Ъ}{{\selectfont\char218}}1
	{Ы}{{\selectfont\char219}}1
	{Ь}{{\selectfont\char220}}1
	{Э}{{\selectfont\char221}}1
	{Ю}{{\selectfont\char222}}1
	{Я}{{\selectfont\char223}}1
	{і}{{\selectfont\char105}}1
	{ї}{{\selectfont\char168}}1
	{є}{{\selectfont\char185}}1
	{ґ}{{\selectfont\char160}}1
	{І}{{\selectfont\char73}}1
	{Ї}{{\selectfont\char136}}1
	{Є}{{\selectfont\char153}}1
	{Ґ}{{\selectfont\char128}}1
	{\{}{{{\color{brackets}\{}}}1 % Цвет скобок {
	{\}}{{{\color{brackets}\}}}}1 % Цвет скобок }
}

\setcounter{tocdepth}{1}

\begin{document}

\begin{titlepage}
\newpage

\

Тут титульник
\end{titlepage}

\newpage
\tableofcontents
\setcounter{page}{2}


\newpage\section{Случайный лес для задачи классификации} 
	Random Forest ("Случайный лес") является композицией (ансамблем) множества решающих деревьев, что позволяет снизить проблему переобучения и повысить точность в сравнении с одним деревом. Прогноз получается в результате агрегирования ответов множества деревьев. Тренировка деревьев происходит независимо друг от друга (на разных подмножествах), что не просто решает проблему построения одинаковых деревьев на одном и том же наборе данных, но и делает этот алгоритм весьма удобным для применения в системах распределённых вычислений.
	
	\vspace{0.5cm}
	
	
	
\newpage\section{Цель лабораторной работы} 
	Цели: 
	\vspace{0.5cm}
	
	Получить практические навыки по применению алгоритма случайного леса
	
	\vspace{0.5cm}
	Задачи: 
	
	\vspace{0.5cm}
	1. Применить к датасету Titanic алгоритм случайного леса.
	
	\vspace{0.5cm}
	2. Провести ряд экспериментов с целью получения лучших значений гиперпараметров и выбора наилучшего сочетания признаков.
	
	
\newpage\section{Инструменты} 
	В качестве инструментов для выполнения поставленной цели был выбран язык Python и библиотеки Sсikit-learn и pandas.
	Бибилотека pandas была использована для подготовки датасета к будущему использованию.
	
	\vspace{0.5cm}
	Библиотека Sсikit-learn была использована для построения дерева решений. Для этого использовалась функция RandomForestClassifier. 
	
	\vspace{0.5cm}
	Основные параметры класса sklearn.ensemble.RandomForestClassifier.
	
	\vspace{0.5cm}
	max\_depth – максимальная глубина дерева. По умолчанию не ограничена.
	
	\vspace{0.5cm}
	max\_features — максимальное число признаков, по которым ищется лучшее разбиение в дереве. Можно указать конкретное число или процент признаков, либо выбрать из доступных значений: "auto" (все признаки), "sqrt", "log2". По умолчанию стоит "auto".
	
	\vspace{0.5cm}
	min\_samples\_leaf - минимальное число объектов в листе. Можно задать числом или процентом от общего числа объектов (по умолчанию — 1).
	
	\vspace{0.5cm}
	n\_estimators - количество деревьев в лесу (по умолчанию 10).
	
	\vspace{0.5cm}
	random\_state - начальное значение для генерации случайных чисел.

	
\newpage\section{Эксперименты}
	Для того, чтобы вывести при каких параметрах функция RandomForestClassifier дает наиболее точное предсказание, необходимо провести ряд экспериментов с разными значениями параметров. 
	
	\vspace{0.5cm}
	Рядом экспериментов было выявлено, что наилучшая точность достигается при random\_state равном 5-ти. Поэтому примем данный параметр равным пяти во всех экспериментах.
	
	\\Еще не доделано
	
	\vspace{0.5cm}
	Таблица 1 - Точность предстказаний в зависимости от параметров.
\begin{longtable}{|p{1cm}|p{9cm}|p{3cm}|}
\hline
№ & Параметры & Точность (в процентах) \\ 
\hline 
1 & Параметры по умолчанию & 77.039 \\
\hline
2 & max\_depth=10; max\_features=2; min\_samples\_split=10; min\_samples\_leaf=50; & 100.0 \\
\hline 
3 & max\_depth=10; max\_features=2; min\_samples\_split=40; min\_samples\_leaf=10; & 91.843 \\
\hline 
4 & max\_depth=10; max\_features=3; min\_samples\_split=2; min\_samples\_leaf=1; & 80.664 \\
\hline 
5 & max\_depth=10; max\_features=5; min\_samples\_split=2; min\_samples\_leaf=1; & 81.873 \\
\hline
6 & max\_depth=10; max\_features=5; min\_samples\_split=50; min\_samples\_leaf=10; & 87.311 \\
\hline  
7 & max\_depth=10, max\_features=5; min\_samples\_split=10; min\_samples\_leaf=50; & 88.822 \\
\hline
8 & max\_depth=100, max\_features=2; min\_samples\_split=10; min\_samples\_leaf=50; & 100.0 \\
\hline 
9 & max\_depth=100, max\_features=2; min\_samples\_split=10; min\_samples\_leaf=50; & 95.166 \\
\hline
10 & max\_depth=100, max\_features=3; min\_samples\_split=2; min\_samples\_leaf=1; & 77.64 \\
\hline
11 & max\_depth=100, max\_features=5; min\_samples\_split=2; min\_samples\_leaf=1; & 77.34 \\
\hline
12 & max\_depth=100, max\_features=5; min\_samples\_split=50; min\_samples\_leaf=10; & 87.311 \\
\hline
13 & max\_depth=100, max\_features=5; min\_samples\_split=10; min\_samples\_leaf=50; & 88.822 \\
\hline 
14 & max\_depth=5; max\_features=2; min\_samples\_split=10; min\_samples\_leaf=50; & 100.0 \\
\hline 
15 & max\_depth=5; max\_features=2; min\_samples\_split=40; min\_samples\_leaf=10;  & 97.28 \\
\hline 
16 & max\_depth=5, max\_features=3; min\_samples\_split=2; min\_samples\_leaf=1; & 95.77 \\
\hline
17 & max\_depth=5, max\_features=5; min\_samples\_split=2; min\_samples\_leaf=1; & 94.562 \\
\hline
18 & max\_depth=5; max\_features=5; min\_samples\_split=50; min\_samples\_leaf=10;  & 87.311 \\
\hline
19 & max\_depth=5, max\_features=5; min\_samples\_split=10; min\_samples\_leaf=50; & 88.822 \\
\hline 


\hline
\end{longtable}


\newpage\section{Итог}
	В ходе проведения экперименты были выявлены наиболее оптимальные сочетания основных параметров функции DecisionTreeClassifier из Skikit-learn. При минимальном количестве экземпляров в листе во всех трех группах получались наиболее слабые результаты. Исходя их того, что наихудшие данные были получены при максимальной глубине дерева, можно сделать вывод, что вероятно произошло переобучение модели, вследствии таких ограничений. 
	
	\vspace{0.5cm}
	Лучшие результаты были получены при минимальном количестве экземпрляров в листе, равном 50. При этом, глубина дерева не оказывала значительного влиния. Лучшие результаты были достигунты при использовании малого числа признаков. 
	
	\vspace{0.5cm}
	При этом также, хорошие результаты показали эксперименты с наименьшей глубиной дерева (равной 5).
	
	\vspace{0.5cm}
	В итоге, можно сделать вывод, что для используемого набора данных, при допускании большой глубины дерева и самого малого из возможных числа экземпляров в листе, результаты ухудшаются, что может свидетельствовать о переобучении модели.
	
	
\end{document}
