\documentclass[a4paper,12pt]{article}

\usepackage[utf8x]{inputenc}
\usepackage[english, russian]{babel}

\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{misccorr}
\usepackage{indentfirst}
\usepackage{amsmath}
%\usepackage{fancynum}


\usepackage{listings}
\usepackage{xcolor}

\usepackage{fullpage}

\usepackage[labelsep=endash,
		    margin=10pt, 
		    justification = centerlast, 
		    format = hang,
		    singlelinecheck=false
		    ]{caption}

\exhyphenpenalty=10000
\doublehyphendemerits=10000
\finalhyphendemerits=5000

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\newcommand{\tracking}[2]{#2}
\input{letterspacing.tex}\renewcommand{\tracking}[2]{\mbox{\letterspace to #1\naturalwidth{#2}}}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=t,
    keepspaces=true,
    numbers=left,
    numbersep=10pt,
    showspaces=false,
    showstringspaces=false
    showtabs=false,
    tabsize=4,
    frame=tb
}
 
\lstset{style=mystyle}

\usepackage{color}
\usepackage{xcolor}
\usepackage{listings}
 
% Цвета для кода
 
\definecolor{string}{HTML}{B40000} % цвет строк в коде
\definecolor{comment}{HTML}{008000} % цвет комментариев в коде
\definecolor{keyword}{HTML}{1A00FF} % цвет ключевых слов в коде
\definecolor{morecomment}{HTML}{8000FF} % цвет include и других элементов в коде
\definecolor{сaptiontext}{HTML}{FFFFFF} % цвет текста заголовка в коде
\definecolor{сaptionbk}{HTML}{999999} % цвет фона заголовка в коде
\definecolor{bk}{HTML}{FFFFFF} % цвет фона в коде
\definecolor{frame}{HTML}{999999} % цвет рамки в коде
\definecolor{brackets}{HTML}{B40000} % цвет скобок в коде
 

%%% Отображение кода %%%
 
% Настройки отображения кода
 
\lstset{
	%morekeywords={*,...}, % если хотите добавить ключевые слова, то добавляйте	 
	% Настройки отображения     
	breaklines=false, % Перенос длинных строк
	% Для отображения русского языка
	extendedchars=true,
	literate={Ö}{{\"O}}1
	{Ä}{{\"A}}1
	{Ü}{{\"U}}1
	{ß}{{\ss}}1
	{ü}{{\"u}}1
	{ä}{{\"a}}1
	{ö}{{\"o}}1
	{~}{{\textasciitilde}}1
	{а}{{\selectfont\char224}}1
	{б}{{\selectfont\char225}}1
	{в}{{\selectfont\char226}}1
	{г}{{\selectfont\char227}}1
	{д}{{\selectfont\char228}}1
	{е}{{\selectfont\char229}}1
	{ё}{{\"e}}1
	{ж}{{\selectfont\char230}}1
	{з}{{\selectfont\char231}}1
	{и}{{\selectfont\char232}}1
	{й}{{\selectfont\char233}}1
	{к}{{\selectfont\char234}}1
	{л}{{\selectfont\char235}}1
	{м}{{\selectfont\char236}}1
	{н}{{\selectfont\char237}}1
	{о}{{\selectfont\char238}}1
	{п}{{\selectfont\char239}}1
	{р}{{\selectfont\char240}}1
	{с}{{\selectfont\char241}}1
	{т}{{\selectfont\char242}}1
	{у}{{\selectfont\char243}}1
	{ф}{{\selectfont\char244}}1
	{х}{{\selectfont\char245}}1
	{ц}{{\selectfont\char246}}1
	{ч}{{\selectfont\char247}}1
	{ш}{{\selectfont\char248}}1
	{щ}{{\selectfont\char249}}1
	{ъ}{{\selectfont\char250}}1
	{ы}{{\selectfont\char251}}1
	{ь}{{\selectfont\char252}}1
	{э}{{\selectfont\char253}}1
	{ю}{{\selectfont\char254}}1
	{я}{{\selectfont\char255}}1
	{А}{{\selectfont\char192}}1
	{Б}{{\selectfont\char193}}1
	{В}{{\selectfont\char194}}1
	{Г}{{\selectfont\char195}}1
	{Д}{{\selectfont\char196}}1
	{Е}{{\selectfont\char197}}1
	{Ё}{{\"E}}1
	{Ж}{{\selectfont\char198}}1
	{З}{{\selectfont\char199}}1
	{И}{{\selectfont\char200}}1
	{Й}{{\selectfont\char201}}1
	{К}{{\selectfont\char202}}1
	{Л}{{\selectfont\char203}}1
	{М}{{\selectfont\char204}}1
	{Н}{{\selectfont\char205}}1
	{О}{{\selectfont\char206}}1
	{П}{{\selectfont\char207}}1
	{Р}{{\selectfont\char208}}1
	{С}{{\selectfont\char209}}1
	{Т}{{\selectfont\char210}}1
	{У}{{\selectfont\char211}}1
	{Ф}{{\selectfont\char212}}1
	{Х}{{\selectfont\char213}}1
	{Ц}{{\selectfont\char214}}1
	{Ч}{{\selectfont\char215}}1
	{Ш}{{\selectfont\char216}}1
	{Щ}{{\selectfont\char217}}1
	{Ъ}{{\selectfont\char218}}1
	{Ы}{{\selectfont\char219}}1
	{Ь}{{\selectfont\char220}}1
	{Э}{{\selectfont\char221}}1
	{Ю}{{\selectfont\char222}}1
	{Я}{{\selectfont\char223}}1
	{і}{{\selectfont\char105}}1
	{ї}{{\selectfont\char168}}1
	{є}{{\selectfont\char185}}1
	{ґ}{{\selectfont\char160}}1
	{І}{{\selectfont\char73}}1
	{Ї}{{\selectfont\char136}}1
	{Є}{{\selectfont\char153}}1
	{Ґ}{{\selectfont\char128}}1
	{\{}{{{\color{brackets}\{}}}1 % Цвет скобок {
	{\}}{{{\color{brackets}\}}}}1 % Цвет скобок }
}

\setcounter{tocdepth}{1}

\begin{document}

\begin{titlepage}
\newpage

\

Тут титульник
\end{titlepage}

\newpage
\tableofcontents
\setcounter{page}{2}



\newpage\section{Наивный байесовский классификатор и логистическая регрессия} 
	Наивный байесовский классификатор — простой вероятностный классификатор, основанный на применении теоремы Байеса со строгими (наивными) предположениями о независимости.

	\vspace{0.5cm}
	Наивный Байесовский классификатор один из самых простых из алгоритмов классификации. Тем не менее, очень часто он работает не хуже, а то и лучше более сложных алгоритмов. Достоинством наивного байесовского классификатора является малое количество данных для обучения, необходимых для оценки параметров, требуемых для классификации.

	\vspace{0.5cm}
	Логистическая регрессия является одним из статистических методов классификации с использованием линейного дискриминанта Фишера. В отличие от обычной регрессии, в методе логистической регрессии не производится предсказание значения числовой переменной исходя из выборки исходных значений. Вместо этого, значением функции является вероятность того, что данное исходное значение принадлежит к определенному классу. 
	
	\vspace{0.5cm}
	Эта граница задается в зависимости от имеющихся исходных данных и обучающего алгоритма. Чтобы все работало, точки исходных данных должны разделяться линейной границей на две вышеупомянутых области. Если точки исходных данных удовлетворяют этому требованию, то их можно назвать линейно разделяемыми. 
	
	
	
\newpage\section{Цель лабораторный работы} 
	Цели: 
	\vspace{0.5cm}
	
	Получить практические навыки по применению статистических методов классификации
	
	\vspace{0.5cm}
	Задачи: 
	
	\vspace{0.5cm}
	1. Применить к датасету Titanic наивный байесовский классификатор.
	
	\vspace{0.5cm}
	2. Применить к датасету Titanic логистическую регрессию.

	\vspace{0.5cm}
	3. Провести ряд экспериментов с целью получения лучших значений гиперпараметров и выбора наилучшего сочетания признаков.
	
\newpage\section{Инструменты} 
	В качестве инструментов для выполнения поставленной цели был выбран язык Python и библиотеки scikit-learn и Pandas.
	Бибилотека Pandas была использована для подготовки датасета к будущему использованию.
	
	\vspace{0.5cm}
	Библиотека scikit-learn была использована  в качестве реализации наивного байесовского классификатора и логической регрессии. Для этого использовались классы GaussianNB, BernoulliNB, MultinomialNB и LogisticRegression. GaussianNB, BernoulliNB, MultinomialNB - классы относящиеся к наивному байесовскому классификатору. Они не имеют значимых параметров.
	
	\vspace{0.5cm}
	Gaussian (нормальное распределение). Модель данного типа используется в случае непрерывных признаков и предполагает, что значения признаков имеют нормальное распределение.
	
	\vspace{0.5cm}
	Multinomial (мультиномиальное распределение). Используется в случае дискретных признаков. Например, в задаче классификации текстов признаки могут показывать, сколько раз каждое слово встречается в данном тексте.
	
	\vspace{0.5cm}
	Bernoulli (распределение Бернулли). Используется в случае двоичных дискретных признаков (могут принимать только два значения: 0 и 1). Например, в задаче классификации текстов с применением подхода «мешок слов» (bag of words) бинарный признак определяет присутствие (1) или отсутствие (0) данного слова в тексте.
	
	\vspace{0.5cm}
	LogisticRegression - класс, реализующий логическую регрессию.
	
	\vspace{0.5cm}
	Основные параметры Logistic Regression:
	
	\vspace{0.5cm}
	C - параметр регуляризации (по умолчанию 1);

	\vspace{0.5cm}
	tol - требуемая точность (по умолчанию 0.001);
	
	\vspace{0.5cm}
	random\_state - начальное значение для генерации случайных чисел. Необходим для повторимости экспериментов. Примем его равным нулю.


	
\newpage\section{Эксперименты}
	Для того, чтобы выяснить, какая из моделей, реализующих Байесовский класификатор, точнее, проведем ряд экспериментов  с использованием всех трех моделей, которые есть в scikit-learn. Результаты отображены в Таблице 1.
	
	\vspace{0.5cm}
	Таблица 1 - Точность прогнозов наивного байесовского классификатора в зависимости от параметров.
\begin{longtable}{|p{1cm}|p{9cm}|p{3cm}|}
\hline 
№ & Параметры & Точность (в процентах) \\ 
\hline 
1 & GaussianNB & 91.54 \\
\hline
2 & BernoulliNB & 100.0 \\
\hline 
3 & MultinomialNB & 65.55 \\
\hline
\end{longtable}	
	
	Для того, чтобы выяснить при каких параметрах функция Logistic Regression дает наиболее точную классификацию, необходимо провести ряд экспериментов с разными значениями параметров. Результаты приведены в Таблице 2.
	
	\vspace{0.5cm}
	Таблица 2 - Точность прогнозов логистической регресии в зависимости от параметров.
\begin{longtable}{|p{1cm}|p{9cm}|p{3cm}|}
\hline 
№ & Параметры & Точность (в процентах) \\ 
\hline 
1 & Параметры по умолчанию & 96.163 \\
\hline
2 & С=1; tol=0.0001; & 93.957 \\
\hline 
3 & С=1; tol=0.01; & 93.353 \\
\hline 
4 &  С=1; tol=0.5;  & 63.44 \\
\hline 
5 & С=1; tol=0.99;  & 63.44 \\
\hline
2 & С=3; tol=0.0001; & 93.353 \\
\hline 
3 & С=3; tol=0.01; & 95.468 \\
\hline 
4 &  С=3; tol=0.5;  & 63.444 \\
\hline 
5 & С=3; tol=0.99;  & 63.444 \\
\hline
2 & С=0.5; tol=0.0001; & 94.864 \\
\hline 
3 & С=0.5; tol=0.01; & 95.166 \\
\hline 
4 & С=0.5; tol=0.5;  & 63.444 \\
\hline 
5 & С=0.5; tol=0.99;  & 63.444 \\
\hline 
\end{longtable}


\newpage\section{Итог}
	По проведенным экспериментам, касательно наивного байесовского классификатора, можно увидеть, что наилучшей точностью для датасета "Титаник" обладает распределение Бернулли.
	
	\vspace{0.5cm}
	Наилучшие результаты по логистической регрессии были получены при точности в 0.0001 и параметре регуляризации равном 0.5. Из чего можно предположить, что при больших значениях параметра, происходило переобучение, так как штраф, заключающийся в регуляризаторе, за излишнюю перенастройку классификатора в получался слишком маленьким, и не оказывал нужного влияния на функцию обучения.

\end{document}
