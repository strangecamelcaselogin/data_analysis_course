\documentclass[a4paper,12pt]{article}

\usepackage[utf8x]{inputenc}
\usepackage[english, russian]{babel}

\usepackage{tabularx}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{misccorr}
\usepackage{indentfirst}
\usepackage{amsmath}
%\usepackage{fancynum}


\usepackage{listings}
\usepackage{xcolor}

\usepackage{fullpage}

\usepackage[labelsep=endash,
		    margin=10pt, 
		    justification = centerlast, 
		    format = hang,
		    singlelinecheck=false
		    ]{caption}

\exhyphenpenalty=10000
\doublehyphendemerits=10000
\finalhyphendemerits=5000

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\newcommand{\tracking}[2]{#2}
\input{letterspacing.tex}\renewcommand{\tracking}[2]{\mbox{\letterspace to #1\naturalwidth{#2}}}
 
\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},
    commentstyle=\color{codegreen},
    keywordstyle=\color{blue},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,
    breaklines=true,
    captionpos=t,
    keepspaces=true,
    numbers=left,
    numbersep=10pt,
    showspaces=false,
    showstringspaces=false
    showtabs=false,
    tabsize=4,
    frame=tb
}
 
\lstset{style=mystyle}

\usepackage{color}
\usepackage{xcolor}
\usepackage{listings}
 
% Цвета для кода
 
\definecolor{string}{HTML}{B40000} % цвет строк в коде
\definecolor{comment}{HTML}{008000} % цвет комментариев в коде
\definecolor{keyword}{HTML}{1A00FF} % цвет ключевых слов в коде
\definecolor{morecomment}{HTML}{8000FF} % цвет include и других элементов в коде
\definecolor{сaptiontext}{HTML}{FFFFFF} % цвет текста заголовка в коде
\definecolor{сaptionbk}{HTML}{999999} % цвет фона заголовка в коде
\definecolor{bk}{HTML}{FFFFFF} % цвет фона в коде
\definecolor{frame}{HTML}{999999} % цвет рамки в коде
\definecolor{brackets}{HTML}{B40000} % цвет скобок в коде
 

%%% Отображение кода %%%
 
% Настройки отображения кода
 
\lstset{
	%morekeywords={*,...}, % если хотите добавить ключевые слова, то добавляйте	 
	% Настройки отображения     
	breaklines=false, % Перенос длинных строк
	% Для отображения русского языка
	extendedchars=true,
	literate={Ö}{{\"O}}1
	{Ä}{{\"A}}1
	{Ü}{{\"U}}1
	{ß}{{\ss}}1
	{ü}{{\"u}}1
	{ä}{{\"a}}1
	{ö}{{\"o}}1
	{~}{{\textasciitilde}}1
	{а}{{\selectfont\char224}}1
	{б}{{\selectfont\char225}}1
	{в}{{\selectfont\char226}}1
	{г}{{\selectfont\char227}}1
	{д}{{\selectfont\char228}}1
	{е}{{\selectfont\char229}}1
	{ё}{{\"e}}1
	{ж}{{\selectfont\char230}}1
	{з}{{\selectfont\char231}}1
	{и}{{\selectfont\char232}}1
	{й}{{\selectfont\char233}}1
	{к}{{\selectfont\char234}}1
	{л}{{\selectfont\char235}}1
	{м}{{\selectfont\char236}}1
	{н}{{\selectfont\char237}}1
	{о}{{\selectfont\char238}}1
	{п}{{\selectfont\char239}}1
	{р}{{\selectfont\char240}}1
	{с}{{\selectfont\char241}}1
	{т}{{\selectfont\char242}}1
	{у}{{\selectfont\char243}}1
	{ф}{{\selectfont\char244}}1
	{х}{{\selectfont\char245}}1
	{ц}{{\selectfont\char246}}1
	{ч}{{\selectfont\char247}}1
	{ш}{{\selectfont\char248}}1
	{щ}{{\selectfont\char249}}1
	{ъ}{{\selectfont\char250}}1
	{ы}{{\selectfont\char251}}1
	{ь}{{\selectfont\char252}}1
	{э}{{\selectfont\char253}}1
	{ю}{{\selectfont\char254}}1
	{я}{{\selectfont\char255}}1
	{А}{{\selectfont\char192}}1
	{Б}{{\selectfont\char193}}1
	{В}{{\selectfont\char194}}1
	{Г}{{\selectfont\char195}}1
	{Д}{{\selectfont\char196}}1
	{Е}{{\selectfont\char197}}1
	{Ё}{{\"E}}1
	{Ж}{{\selectfont\char198}}1
	{З}{{\selectfont\char199}}1
	{И}{{\selectfont\char200}}1
	{Й}{{\selectfont\char201}}1
	{К}{{\selectfont\char202}}1
	{Л}{{\selectfont\char203}}1
	{М}{{\selectfont\char204}}1
	{Н}{{\selectfont\char205}}1
	{О}{{\selectfont\char206}}1
	{П}{{\selectfont\char207}}1
	{Р}{{\selectfont\char208}}1
	{С}{{\selectfont\char209}}1
	{Т}{{\selectfont\char210}}1
	{У}{{\selectfont\char211}}1
	{Ф}{{\selectfont\char212}}1
	{Х}{{\selectfont\char213}}1
	{Ц}{{\selectfont\char214}}1
	{Ч}{{\selectfont\char215}}1
	{Ш}{{\selectfont\char216}}1
	{Щ}{{\selectfont\char217}}1
	{Ъ}{{\selectfont\char218}}1
	{Ы}{{\selectfont\char219}}1
	{Ь}{{\selectfont\char220}}1
	{Э}{{\selectfont\char221}}1
	{Ю}{{\selectfont\char222}}1
	{Я}{{\selectfont\char223}}1
	{і}{{\selectfont\char105}}1
	{ї}{{\selectfont\char168}}1
	{є}{{\selectfont\char185}}1
	{ґ}{{\selectfont\char160}}1
	{І}{{\selectfont\char73}}1
	{Ї}{{\selectfont\char136}}1
	{Є}{{\selectfont\char153}}1
	{Ґ}{{\selectfont\char128}}1
	{\{}{{{\color{brackets}\{}}}1 % Цвет скобок {
	{\}}{{{\color{brackets}\}}}}1 % Цвет скобок }
}

\setcounter{tocdepth}{1}

\begin{document}

\begin{titlepage}
\newpage

\

Тут титульник
\end{titlepage}

\newpage
\tableofcontents
\setcounter{page}{2}


\newpage\section{Деревья решений} 
	Дерево принятия решений (также может называться деревом классификации или регрессионным деревом) — средство поддержки принятия решений, использующееся в статистике и анализе данных для прогнозных моделей. 
	
	\vspace{0.5cm}
	На ребрах («ветках») дерева решения записаны атрибуты, от которых зависит целевая функция, в «листьях» записаны значения целевой функции, а в остальных узлах — атрибуты, по которым различаются случаи. Чтобы классифицировать новый случай, надо спуститься по дереву до листа и выдать соответствующее значение. 
	
	\vspace{0.5cm}
	Если при классификации в листах стоят результирующие классы, при регрессии же стоит значение целевой функции.
	
	
	
\newpage\section{Цель лабораторной работы} 
	Цели: 
	\vspace{0.5cm}
	
	Получить практические навыки по применению алгоритма дерева решений.
	
	\vspace{0.1cm}
	Задачи: 
	
	\vspace{0.1cm}
	1. Сохранить локально датасет Titanic.
	
	\vspace{0.1cm}
	2. Применить к датасету Titanic метод деревьев решений.
	
	\vspace{0.1cm}
	3. Провести ряд экспериментов с целью получения лучших значений гиперпараметров и выбора наилучшего сочетания признаков.
	
\newpage\section{Инструменты} 
	В качестве инструментов для выполнения поставленной цели был выбран язык Python и библиотека для машинного обучения sсikit-learn и библиотека для организации работы с исходными данными - Pandas.
	
	\vspace{0.5cm}
	Библиотека scikit-learn была использована для построения дерева решений. Для этого использовался DecisionTreeClassifier из пакета sklearn.tree. DecisionTreeClassifier использует алгоритм CART.
	
	\vspace{0.5cm}
	Основные входные параметры объекта DecisionTreeClassifier.
	
	\vspace{0.5cm}
	max\_depth – максимальная глубина дерева. По умолчанию не ограничена.
	
	\vspace{0.5cm}
	max\_features — максимальное число признаков, по которым ищется лучшее разбиение в дереве. Можно указать конкретное число или процент признаков, либо выбрать из доступных значений: "auto" (все признаки), "sqrt", "log2". По умолчанию стоит "auto".
	
	\vspace{0.5cm}
	min\_samples\_split - минимальное количество объектов, необходимое для разделения внутреннего узла. Можно задать числом или процентом от общего числа образцов (по умолчанию — 2)
	
	\vspace{0.5cm}
	min\_samples\_leaf - минимальное число образцов для образования листа. Можно задать абсолютно или в процентном соотношении (по умолчанию — 1).
	
	\vspace{0.5cm}
	random\_state - начальное значение для генерации случайных чисел.


	
\newpage\section{Эксперименты}
	Для того, чтобы вывести при каких параметрах функция DecisionTreeClassifier дает наиболее точное предсказание, необходимо провести ряд экспериментов с разными значениями параметров. 
	
	\vspace{0.5cm}
	Для повторимости проводимых эксперементов у всех моделей параметр random\_state равен 5. 
	
	\vspace{0.5cm}
	Таблица 1 - Точность прогноза классификатора в зависимости от гиперпараметров.
\begin{longtable}{|p{1cm}|p{9cm}|p{3cm}|}
\hline
№ & Параметры & Точность (в процентах) \\ 
\hline 
1 & Параметры по умолчанию & 74.92 \\
\hline
2 & max\_depth=10; max\_features=2; min\_samples\_split=10; min\_samples\_leaf=50; & 95.77 \\
\hline 
3 & max\_depth=10; max\_features=3; min\_samples\_split=10; min\_samples\_leaf=50; & 100 \\
\hline 
4 & max\_depth=10; max\_features=2; min\_samples\_split=40; min\_samples\_leaf=10; & 90.03 \\
\hline 
5 & max\_depth=10; max\_features=3; min\_samples\_split=2; min\_samples\_leaf=1; & 72.81 \\
\hline 
6 & max\_depth=10; max\_features=None; min\_samples\_split=25; min\_samples\_leaf=10; & 85.8 \\
\hline
7 & max\_depth=10; max\_features=6; min\_samples\_split=25; min\_samples\_leaf=10; & 85.8 \\

\hline
\end{longtable}


\newpage\section{Итог}
	В ходе проведения экпериментов были выявлены наиболее оптимальные сочетания входных параметров модели DecisionTreeClassifier из scikit-learn. При минимальном количестве экземпляров в листе во всех трех группах получались наиболее слабые результаты. Исходя их того, что наихудшие данные были получены при максимальной глубине дерева, можно сделать вывод, что вероятно произошло переобучение модели, вследствии таких ограничений. 
	
	\vspace{0.5cm}
	Лучшие результаты были получены при минимальном количестве экземпрляров в листе, равном 50. При этом, глубина дерева не оказывала значительного влиния из-за отсутствия возможности дерева расти в глубь. При таких параметрах наибольшую значимость оказывало максимальное количество признаков, птолько при использовании малого числа признаков (2 и 3) модель дает лучшие результаты. 
	
	\vspace{0.5cm}
	При этом также, хорошие результаты показали эксперименты с наименьшей глубиной дерева (равной 5).
	
	\vspace{0.5cm}
	В итоге, можно сделать вывод, что для используемого набора данных, при допускании большой глубины дерева и самого малого из возможных числа экземпляров в листе, результаты ухудшаются, что может свидетельствовать о переобучении модели.
	
	
\end{document}
